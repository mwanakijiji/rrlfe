{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a wrapper for a command line operation using Robospect and Ken Carrell's \n",
    "# normalization script to\n",
    "\n",
    "# A. REPRODUCE our steps for finding a, b, c, d\n",
    "# B. APPLY our solution to any other spectra to find [Fe/H]\n",
    "\n",
    "# written by E.S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from subprocess import call\n",
    "from subprocess import Popen\n",
    "import shlex\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, os.path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pylab import * \n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class norm_spec:\n",
    "    \n",
    "    ##############################################################################\n",
    "    # STEP 1: NORMALIZE SPECTRA (applicable to A and B)\n",
    "    ##############################################################################\n",
    "    \n",
    "    # this is a superclass that will be inherited by other classes\n",
    "    \n",
    "    def __init__(self, input_file):\n",
    "        self.smoothing = 22 # smoothing applied by Carrell's normalization code\n",
    "        self.input_file = input_file\n",
    "        \n",
    "    def __call__(self):\n",
    "        \n",
    "        # compile Carrell's normalization code\n",
    "        # (see carrell_readme.txt)\n",
    "        \n",
    "        # Carrell's C code should already be compiled by setup.py. But here are the manual commands for posterity:\n",
    "        #normzn_compile1 = shlex.split(\"g++ -o bkgrnd bkgrnd.cc\")\n",
    "        #normzn_compile2 = subprocess.Popen(normzn_compile1) # run\n",
    "        \n",
    "        # run the normalization routine on the data\n",
    "        normzn_run1 = shlex.split(\"./bkgrnd --smooth \"+str(self.smoothing)+\" \"+self.input_file) # self.input_file can be 'in.data'\n",
    "        normzn_run2 = subprocess.Popen(normzn_run1, stdout=subprocess.PIPE, stderr=subprocess.PIPE) # run and capture output\n",
    "                                  \n",
    "        # make list of all output files\n",
    "        dir_name = \"test_output/\"\n",
    "        list_output_files = [name for name in os.listdir(dir_name) if os.path.isfile(os.path.join(dir_name, name))]\n",
    "                                  \n",
    "        # divide the second column of the output files (empirical) with the third (normalization) \n",
    "        header = ['WAVELENGTH', 'FLUX'] # headers of output file that Robospect will use (see Robospect user manual)\n",
    "        for filenum in range(0,len(list_output_files)):\n",
    "            df = pd.read_csv('test_output/'+str(list_output_files[filenum]), delim_whitespace=True, header=None)\n",
    "            df['WAVELENGTH'] = df[0]\n",
    "            df['FLUX'] = np.divide(df[1],df[2]) # normalize\n",
    "            df.to_csv('test_normzed_output/output.csv', columns = header, index = False, sep = ' ') # write out file \n",
    "            del df                          \n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example: run normalization of raw data\n",
    "\n",
    "do_normzn = norm_spec(\"input_file\") # initialize class instance\n",
    "# do_normzn.smoothing = 22 # can overload default smoothing\n",
    "do_normzn() # call it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class generate_synthetic_spec(norm_spec):\n",
    "    \n",
    "    ##############################################################################\n",
    "    # STEP 2: TAKE EMPIRICAL SPECTRA AND CHANGE NOISE CHARACTERISTICS TO GENERATE SYNTHETIC SPECTRA\n",
    "    # THIS WILL INVOLVE RUNNING RW'S FORTRAN SCRIPT (applicable to A)\n",
    "    ##############################################################################\n",
    "    \n",
    "    # inherits norm_spec class for normalizing spectra\n",
    "    \n",
    "    # normalize the synthetic spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subprocess.call([\"gfortran\",\"-o\",\"hello\",\"junk.f90\"]) # create\n",
    "#subprocess.call([\"./hello\"])   # subprocess.check_output([\"./hello\"]) lets you see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_robospect():\n",
    "### THIS NOT WORKING!!!\n",
    "\n",
    "    ##############################################################################\n",
    "    # STEP 3: RUN ROBOSPECT ON ANY SPECTRA AND WRITE OUT EW VALUES AS *.c.dat FILES (applicable to A and B)\n",
    "    ##############################################################################\n",
    "    \n",
    "    # for applying to synthetic spectra \n",
    "\n",
    "    # accumulate list of filenames of normalized synthetic spectra\n",
    "    fileNameList = glob.glob(\"../*_*.c.dat\") # (or search whatever other directory the *.c.dat files are in)\n",
    "\n",
    "    # for-loop to write out *.robolines and *.robospect files\n",
    "    for p in fileNameList: \n",
    "        # default command: (-F: find all lines; -P: sets path of output files)\n",
    "        # robospect -F -P rs.out example.dat\n",
    "        args = ['./src/robospect', '-F', '-P', 'rs.out', p]\n",
    "        q = subprocess.call(args, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N.b. Wrapper should catch anything that robospect tries to print to terminal \n",
    "# (3 pipes in any process: StdIn, StdOut, and StdError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class scraper():\n",
    "\n",
    "    ##############################################################################\n",
    "    # STEP 3B: SCRAPE ALL THE EW INFO FROM *.c.dat FILES (applicable to A and B)\n",
    "    ##############################################################################\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # directory containing the directory containing *.c.dat files\n",
    "        self.stem = '/home/../../media/unasemaje/Seagate Expansion Drive/rrlyrae_data_reduction/'\n",
    "        # subdirectory containing the *.c.dat files\n",
    "        self.subdir = 'McDrealiz'\n",
    "        \n",
    "        # get list of filenames without the path\n",
    "        fileListLong = glob.glob(stem+subdir+'/'+'*.fits.robolines')\n",
    "        fileListUnsorted = [os.path.basename(x) for x in fileListLong]\n",
    "        fileList = sorted(fileListUnsorted)\n",
    "        \n",
    "    def __call__(self):\n",
    "        \n",
    "        # sanity check: are the lines listed in order?\n",
    "        def line_check(lineCenters):\n",
    "            if ((lineCenters[0] < 3933.660-10) or (lineCenters[0] > 3933.660+10)): # CaIIK\n",
    "            print('Lines not matching!')\n",
    "            sys.exit  # ... and abort\n",
    "        elif ((lineCenters[1] < 3970.075-10) or (lineCenters[1] > 3970.075+10)): # H-epsilon (close to CaIIH)\n",
    "            print('Lines not matching!')\n",
    "            sys.exit\n",
    "        elif ((lineCenters[2] < 4101.7100-10) or (lineCenters[2] > 4101.7100+10)): # H-delta\n",
    "            print('Lines not matching!')\n",
    "            sys.exit\n",
    "        elif ((lineCenters[3] < 4340.472-10) or (lineCenters[3] > 4340.472+10)): # H-gamma\n",
    "            print('Lines not matching!')\n",
    "            sys.exit\n",
    "        elif ((lineCenters[4] < 4861.290-10) or (lineCenters[4] > 4861.290+10)): # H-beta\n",
    "            print('Lines not matching!')\n",
    "            sys.exit\n",
    "        return\n",
    "    \n",
    "    \n",
    "        # loop over all filenames, extract line data\n",
    "        for t in range(0,len(fileList)):\n",
    "        \n",
    "            # read in Robospect output\n",
    "            df = pd.read_csv(stem+subdir+'/'+fileList[t], header=13, delim_whitespace=True, index_col=False, usecols=np.arange(17))\n",
    "    \n",
    "            # check lines are in the right order\n",
    "            line_check(df['#x0'])\n",
    "    \n",
    "            # add two cols on the left: the filename, and the name of the line\n",
    "            sLength = len(df['mean']) # number of lines (should be 5)\n",
    "            df['file_name'] = pd.Series(fileList[t], index=df.index)\n",
    "            df['synth_spec_name'] = pd.Series(fileList[t].split(\".\")[0], index=df.index) # multiple synthetic spectra correspond to one empirical spectrum\n",
    "            df['empir_spec_name'] = pd.Series(fileList[t].split(\".\")[0][0:-4], index=df.index) # empirical spectrum\n",
    "            #df['star_name'] = pd.Series(fileList[t].split(\"__\")[0], index=df.index)\n",
    "            df['line_name'] = ['CaIIK', 'Heps', 'Hdel', 'Hgam', 'Hbet']\n",
    "    \n",
    "            # get an idea of the progress\n",
    "            clear_output(wait=True)\n",
    "            print('Out of '+str(len(fileList))+' files, '+str(t)+' scraped...')\n",
    "    \n",
    "            # if this is the first list, start a master copy from it to concatenate stuff to it\n",
    "            if (t==0):\n",
    "                dfMaster = df.copy()\n",
    "            else:\n",
    "                dfMaster = pd.concat([dfMaster,df])\n",
    "                del df # clear variable\n",
    "                \n",
    "        # write to csv, while resetting the indices\n",
    "        # note THIS TABLE INCLUDES ALL DATA, GOOD AND BAD\n",
    "        dfMaster_reset = dfMaster.reset_index(drop=True).copy() # this gets shown further down in this notebook\n",
    "        dfMaster.reset_index(drop=True).to_csv(stem+subdir+'/'+subdir+'_largeTable_test.csv') # this is effectively the same, but gets written out\n",
    "        \n",
    "        ## IF WE ARE INTERESTED IN SPECTRA THAT HAVE ALL WELL-FIT LINES\n",
    "        # identify the synthetic spectrum names which have at least one line with a bad fit\n",
    "        badSynthSpectra = dfMaster_reset['synth_spec_name'][np.squeeze(whereRedFlag)]\n",
    "        # remove duplicate names\n",
    "        badSynthSpectra_uniq = badSynthSpectra.drop_duplicates()\n",
    "        # keep only the spectra that have all lines well-fit\n",
    "        dfMaster_reset_dropBadSpectra = dfMaster_reset.where(~dfMaster_reset['synth_spec_name'].isin(badSynthSpectra_uniq))\n",
    "        \n",
    "        # write to csv\n",
    "        # note THIS TABLE HAS SPECTRA WITH ANY BAD ROWS REMOVED\n",
    "        dfMaster_reset_dropBadSpectra.to_csv(subdir+'_largeTable_bad_spectra_removed_test.csv') # this is effectively the same, but gets written out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example: run the scraper \n",
    "\n",
    "do_scrape = scraper() # initialize class instance\n",
    "do_scrape() # call it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class findHK():\n",
    "    \n",
    "    ##############################################################################\n",
    "    # STEP 4: READ IN ROBOSPECT EWS OF SYNTHETIC SPECTRA, RESCALE THEM, AVERAGE THEM, PLOT H-K SPACE (applicable to A and B)\n",
    "    ##############################################################################\n",
    "    \n",
    "    def __init__(self):\n",
    "    \n",
    "        # read in line data\n",
    "        line_data = pd.read_csv('McDrealiz_largeTable_2017jan21_bad_spectra_removed.csv', delim_whitespace=False)\n",
    "        \n",
    "        # initialize arrays: essential info\n",
    "        empir_spec_name_array = []\n",
    "        star_name_array = []\n",
    "        H_data_array = []\n",
    "        K_data_array = []\n",
    "        err_H_data_array = [] \n",
    "        err_K_data_array = []\n",
    "\n",
    "        # initialize arrays: other info\n",
    "        Hbet_data_array = []\n",
    "        err_Hbet_data_array = []\n",
    "        Hgam_data_array = []\n",
    "        err_Hgam_data_array = []\n",
    "        rHgam_data_array = [] # rescaled Hgamma\n",
    "        err_rHgam_data_array = []\n",
    "        Hdel_data_array = []\n",
    "        err_Hdel_data_array = []\n",
    "        Heps_data_array = []\n",
    "        err_Heps_data_array = []\n",
    "        \n",
    "    def __call__(self):\n",
    "        \n",
    "        # make a list of all UNIQUE, EMPIRICAL spectrum names\n",
    "        uniqueSpecNames = line_data.drop_duplicates(subset='empir_spec_name')['empir_spec_name']\n",
    "        \n",
    "        # fit a straight line to Hgam vs Hdel\n",
    "        x_data = line_data['EQW'].where(line_data['line_name'] == 'Hdel').dropna() # Hdel\n",
    "        y_data = line_data['EQW'].where(line_data['line_name'] == 'Hgam').dropna() # Hgam\n",
    "        Hgam = np.copy(y_data)\n",
    "        m,b = np.polyfit(x_data, y_data, 1) # might want errors later, too \n",
    "        \n",
    "        # generate a rescaled Hgam, call it rHgam\n",
    "        rHgam_all = np.divide(np.subtract(Hgam,b),m)\n",
    "        \n",
    "        # prepare data for a plot\n",
    "        # loop over every EMPIRICAL spectrum and assemble SYNTHETIC data into arrays\n",
    "        for p in range(0,len(uniqueSpecNames)):\n",
    "    \n",
    "            # the name of the empirical spectrum being used here\n",
    "            print(np.array(uniqueSpecNames)[p])\n",
    "    \n",
    "            # extract all synthetic data corresponding to this empirical spectrum\n",
    "            data_for_this_empir_spectrum = line_data.where(line_data['empir_spec_name'][0:-4] == np.array(uniqueSpecNames)[p])\n",
    "    \n",
    "            # scrape data\n",
    "            raw_Hbet_data = data_for_this_empir_spectrum['EQW'].where(line_data['line_name'] == 'Hbet')\n",
    "            raw_Hgam_data = data_for_this_empir_spectrum['EQW'].where(line_data['line_name'] == 'Hgam')\n",
    "            raw_Hdel_data = data_for_this_empir_spectrum['EQW'].where(line_data['line_name'] == 'Hdel')\n",
    "            raw_Heps_data = data_for_this_empir_spectrum['EQW'].where(line_data['line_name'] == 'Heps')\n",
    "            raw_K_data = data_for_this_empir_spectrum['EQW'].where(line_data['line_name'] == 'CaIIK')\n",
    "    \n",
    "            # rescale and remove nans\n",
    "            Hbet_data_wnans = np.array(np.copy(raw_Hbet_data))\n",
    "            Hgam_data_wnans = np.array(np.copy(raw_Hgam_data))\n",
    "            Hdel_data_wnans = np.array(np.copy(raw_Hdel_data))\n",
    "            Heps_data_wnans = np.array(np.copy(raw_Heps_data))    \n",
    "            K_data_wnans = np.array(np.copy(raw_K_data))\n",
    "            rHgam_data_wnans = np.array(np.divide(np.subtract(raw_Hgam_data,b),m)) # rescale Hgam EWs\n",
    "    \n",
    "            Hbet_data = Hbet_data_wnans[np.isfinite(Hbet_data_wnans)] # remove nans\n",
    "            Hgam_data = Hgam_data_wnans[np.isfinite(Hgam_data_wnans)]\n",
    "            Hdel_data = Hdel_data_wnans[np.isfinite(Hdel_data_wnans)]\n",
    "            Heps_data = Heps_data_wnans[np.isfinite(Heps_data_wnans)]\n",
    "            rHgam_data = rHgam_data_wnans[np.isfinite(rHgam_data_wnans)]\n",
    "            K_data = K_data_wnans[np.isfinite(K_data_wnans)]\n",
    "    \n",
    "            # get the H-K synthetic data together\n",
    "            balmer_data_allsynthetic_spec = np.mean([Hdel_data,rHgam_data], axis=0) # Balmer EW = 0.5*(Hdel + rHgam)\n",
    "            K_data_allsynthetic_spec = np.copy(K_data)\n",
    "    \n",
    "            # the actual points to plot (or record in a table)\n",
    "            Hbet_data_pt = np.nanmedian(Hbet_data)\n",
    "            Hgam_data_pt = np.nanmedian(Hgam_data)\n",
    "            rHgam_data_pt = np.nanmedian(rHgam_data)\n",
    "            Hdel_data_pt = np.nanmedian(Hdel_data)\n",
    "            Heps_data_pt = np.nanmedian(Heps_data)\n",
    "            balmer_data_pt = np.nanmedian(balmer_data_allsynthetic_spec)\n",
    "            K_data_pt = np.nanmedian(K_data_allsynthetic_spec)\n",
    "    \n",
    "            # the error bars\n",
    "            err_Hbet_data = np.nanstd(Hbet_data)\n",
    "            err_Hgam_data = np.nanstd(Hgam_data)\n",
    "            err_rHgam_data = np.nanstd(rHgam_data)\n",
    "            err_Hdel_data = np.nanstd(Hdel_data)\n",
    "            err_Heps_data = np.nanstd(Heps_data)\n",
    "            err_balmer_data = np.nanstd(balmer_data_allsynthetic_spec)\n",
    "            err_K_data = np.nanstd(K_data_allsynthetic_spec)\n",
    "    \n",
    "            #plt.plot(balmer_data_pt,K_data_pt)\n",
    "            #plt.errorbar(balmer_data_pt, K_data_pt, yerr=err_K_data, xerr=err_balmer_data)\n",
    "\n",
    "            # append data to arrays: essential info\n",
    "            empir_spec_name_array = np.append(empir_spec_name_array,np.array(uniqueSpecNames)[p])\n",
    "            star_name_array = np.append(star_name_array,str(np.array(uniqueSpecNames)[p])[0:-3])\n",
    "            H_data_array = np.append(H_data_array,balmer_data_pt)\n",
    "            err_H_data_array = np.append(err_H_data_array,err_balmer_data)\n",
    "            K_data_array = np.append(K_data_array,K_data_pt)\n",
    "            err_K_data_array = np.append(err_K_data_array,err_K_data)\n",
    "    \n",
    "            # append data to arrays: other info\n",
    "            Hbet_data_array = np.append(Hbet_data_array,Hbet_data_pt)\n",
    "            err_Hbet_data_array = np.append(err_Hbet_data_array,err_Hbet_data)\n",
    "            Hgam_data_array = np.append(Hgam_data_array,Hgam_data_pt)\n",
    "            err_Hgam_data_array = np.append(err_Hgam_data_array,err_Hgam_data)\n",
    "            rHgam_data_array = np.append(rHgam_data_array,err_rHgam_data) # rescaled Hgamma\n",
    "            err_rHgam_data_array = np.append(err_rHgam_data_array,err_rHgam_data)\n",
    "            Hdel_data_array = np.append(Hdel_data_array,Hdel_data_pt)\n",
    "            err_Hdel_data_array = np.append(err_Hdel_data_array,err_Hdel_data)\n",
    "            Heps_data_array = np.append(Heps_data_array,Heps_data_pt)\n",
    "            err_Heps_data_array = np.append(err_Heps_data_array,err_Heps_data)\n",
    "    \n",
    "            # clear some variables\n",
    "            balmer_data_allsynthetic_spec=None \n",
    "            K_data_allsynthetic_spec=None\n",
    "            balmer_data_allsynthetic_spec=None \n",
    "            K_data_allsynthetic_spec=None\n",
    "            \n",
    "        # put everything into a dataframe\n",
    "\n",
    "        d = {'empir_spec_name': empir_spec_name_array, \n",
    "             'star_name': star_name_array,\n",
    "             'Hbet': Hbet_data_array,\n",
    "             'err_Hbet': err_Hbet_data_array,\n",
    "             'Hgam': Hgam_data_array,\n",
    "             'err_Hgam': err_Hgam_data_array,\n",
    "             'Hdel': Hdel_data_array,\n",
    "             'err_Hdel': err_Hdel_data_array,\n",
    "             'Heps': Heps_data_array,\n",
    "             'err_Heps': err_Heps_data_array, \n",
    "             'rHgam': rHgam_data_array,\n",
    "             'err_rHgam': err_rHgam_data_array,  \n",
    "             'balmer': H_data_array,\n",
    "             'err_balmer': err_H_data_array,\n",
    "             'K': K_data_array,\n",
    "             'err_K': err_K_data_array\n",
    "            }     \n",
    "        df_collation = pd.DataFrame(data=d)\n",
    "        \n",
    "        # read in a text file containing phase information\n",
    "        phase_info = pd.read_csv(\"~/Documents/PythonPrograms/all_Python_code/2016_08_27_rrlyrae_metal_fit_emcee_wrapper/eckhart_2ndPass_allSNR_noVXHer_lowAmpPrior.csv\")\n",
    "        \n",
    "        # paste phase info into the table of EWs\n",
    "        phase_array = []\n",
    "        feh_array = []\n",
    "        err_feh_array = []\n",
    "        name_array = []\n",
    "\n",
    "        for q in range(0,len(df_collation)):\n",
    "            name_this_one = phase_info['Spectrum'].where(phase_info['Spectrum'] == df_collation['empir_spec_name'][q]).dropna()\n",
    "            phase_this_one = phase_info['phase'].where(phase_info['Spectrum'] == df_collation['empir_spec_name'][q]).dropna()\n",
    "            feh_this_one = phase_info['FeH'].where(phase_info['Spectrum'] == df_collation['empir_spec_name'][q]).dropna()\n",
    "            err_feh_this_one = phase_info['eFeH'].where(phase_info['Spectrum'] == df_collation['empir_spec_name'][q]).dropna()\n",
    "            name_array = np.append(name_array,name_this_one)\n",
    "            phase_array = np.append(phase_array,phase_this_one)\n",
    "            feh_array = np.append(feh_array,feh_this_one)\n",
    "            err_feh_array = np.append(err_feh_array,err_feh_this_one)\n",
    "        df_collation_real = df_collation.dropna().copy(deep=True) # drop row of nans\n",
    "        df_collation_real['phase'] = phase_array\n",
    "        df_collation_real['FeH'] = feh_array\n",
    "        df_collation_real['eFeH'] = err_feh_array\n",
    "        \n",
    "        # write to csv\n",
    "        df_collation_real.to_csv('more_realistic_EWs_w_phase_test.csv')\n",
    "        \n",
    "        # make plot: each color is a different star, open circles are bad phase region\n",
    "        data_to_plot = pd.read_csv('more_realistic_EWs_w_phase.csv') # read data back in\n",
    "        \n",
    "        # make list of unique star names \n",
    "        unique_star_names = data_to_plot.drop_duplicates(subset=['star_name'])['star_name'].values\n",
    "        \n",
    "        # plot data points\n",
    "        cmap = plt.get_cmap(name='jet')\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "        # loop over every star, overlay the set of points for that star on the plot\n",
    "        for y in range(0,len(unique_star_names)):\n",
    "    \n",
    "            x_data = data_to_plot['balmer'].where(data_to_plot['star_name'] == unique_star_names[y])\n",
    "            y_data = data_to_plot['K'].where(data_to_plot['star_name'] == unique_star_names[y])\n",
    "    \n",
    "            err_x_data = data_to_plot['err_balmer'].where(data_to_plot['star_name'] == unique_star_names[y])\n",
    "            err_y_data = data_to_plot['err_K'].where(data_to_plot['star_name'] == unique_star_names[y])\n",
    "    \n",
    "            # plot, and keep the same color for each star\n",
    "            color_this_star = cmap(float(y)/len(unique_star_names))\n",
    "            ax.errorbar(x_data,y_data,yerr=err_y_data,xerr=err_x_data,linestyle='',fmt='o',markerfacecolor=color_this_star,color = color_this_star)\n",
    "    \n",
    "            x_data_badPhase = x_data.where(np.logical_or(data_to_plot['phase'] > 0.8, data_to_plot['phase'] < 0.05))\n",
    "            y_data_badPhase = y_data.where(np.logical_or(data_to_plot['phase'] > 0.8, data_to_plot['phase'] < 0.05))\n",
    "    \n",
    "            # overplot unfilled markers to denote bad phase region\n",
    "            ax.errorbar(x_data_badPhase,y_data_badPhase,linestyle='',fmt='o',markerfacecolor='white',color = color_this_star)\n",
    "    \n",
    "            # add star name\n",
    "            ax.annotate(unique_star_names[y], xy=(np.array(x_data.dropna())[0], \n",
    "                                          np.array(y_data.dropna())[0]), \n",
    "                xytext=(np.array(x_data.dropna())[0], np.array(y_data.dropna())[0]))\n",
    "    \n",
    "        plt.title('KH plot, using synthetic spectra')\n",
    "        plt.ylabel('CaIIK EW (milliangstrom)')\n",
    "        plt.xlabel('Balmer EW (milliangstrom)')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example: run the findHK\n",
    "\n",
    "do_HK = findHK() # initialize class instance\n",
    "do_HK() # call it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class lit_metallicities():\n",
    "    \n",
    "    ##############################################################################\n",
    "    # STEP X: READ IN LITERATURE METALLICITY VALUES AND RESCALE\n",
    "    ##############################################################################\n",
    "    \n",
    "    def __init__(self):\n",
    "    \n",
    "        stem = \"~/Documents/PythonPrograms/all_Python_code/2018_03_31_rrlyrae_rescale_a_la_chadid/\"\n",
    "        \n",
    "        # Fe/H from Layden+ 1994\n",
    "        layden_feh = pd.read_csv(stem + \"layden_1994_abundances.dat\",delimiter=';')\n",
    "        # RES: \"rather low\"\n",
    "        \n",
    "        # Fe/H Clementini+ 1995\n",
    "        clementini_feh = pd.read_csv(stem + \"clementini_1995_abundances.dat\")\n",
    "\n",
    "        # Fe/H Fernley+ 1996\n",
    "        fernley_feh = pd.read_csv(stem + \"fernley_1996_abundances.dat\")\n",
    "        # RES: 60,000, FeI & FeII, 5900-8100 A\n",
    "        \n",
    "        # log(eps) from Lambert+ 1996\n",
    "        lambert_logeps = pd.read_csv(stem + \"lambert_1996_abundances.dat\")\n",
    "        # RES: ~23,000, FeII + photometric models, 3600-9000 A\n",
    "        \n",
    "        # Fe/H from Wallerstein and Huang 2010, arXiv 1004.2017\n",
    "        wallerstein_feh = pd.read_csv(stem + \"wallerstein_huang_2010_abundances.dat\")\n",
    "        # RES: ~30,000, FeII\n",
    "        \n",
    "        # Fe/H from Chadid+ 2017 (FeI and II lines)\n",
    "        chadid_feh = pd.read_csv(stem + \"chadid_2017_abundances.dat\")\n",
    "        # RES: 38000, FeI & FeII, 3400-9900 A\n",
    "\n",
    "        # Fe/H from Liu+ 2013\n",
    "        liu_feh = pd.read_csv(stem + \"liu_2013_abundances.dat\")\n",
    "        # RES: ~60,000, FeI (& FeII?), 5100-6400 A\n",
    "\n",
    "        # Fe/H from Nemec+ 2013\n",
    "        nemec_feh = pd.read_csv(stem + \"nemec_2013_abundances.dat\")\n",
    "        # RES: ~65,000 or 36,000, FeI & FeII, 5150-5200 A\n",
    "\n",
    "        # Fe/H from Fernley+ 1997\n",
    "        fernley97_feh = pd.read_csv(stem + \"fernley_1997_abundances.dat\",delimiter=';')\n",
    "        # RES: 60,000, two FeII lines, 5900-8100 A\n",
    "\n",
    "        # Fe/H from Solano+ 1997\n",
    "        solano_feh = pd.read_csv(stem + \"solano_1997_abundances.dat\",delimiter=';')\n",
    "        # RES: 22,000 & 19,000, strong FeI lines, 4160-4390 & 4070-4490 A\n",
    "        \n",
    "        # Fe/H from Pacino+ 2015\n",
    "        pacino_feh = pd.read_csv(stem + \"pacino_2015_abundances.dat\") \n",
    "        # RES: >30,000, FeI (weighted average), 4000-8500 A\n",
    "\n",
    "        # Fe/H from Sneden+ 2017\n",
    "        sneden_feh = pd.read_csv(stem + \"sneden_2017_abundances.dat\")\n",
    "        # RES: ~27,000 (at 5000 A), FeI & FeII, 3400-9000 A\n",
    "        \n",
    "        # convert Lambert's values, which are in terms of log(eps)\n",
    "        # FeH = log(epsFe) - log(epsFe,sol)\n",
    "        #     = log(epsFe) - log(NFe,sol/NH,sol)\n",
    "        #     = log(epsFe) - 7.51 # value of 7.51 from Anstee+ 1997, MNRAS\n",
    "        lambert_logeps['feh'] = np.subtract(lambert_logeps['log_eps_fe_spec'], 7.51) \n",
    "        \n",
    "        # average the values in Chadid from FeI and FeII lines\n",
    "        chadid_feh['feh'] = np.mean([chadid_feh[' fehI'].values,chadid_feh[' fehII'].values],axis=0)\n",
    "        \n",
    "        ## ## INCLUDE SINGLE DATA PT FROM KOLENBERG+ 2010? (SEE CHADID+ 2017, FIG. 7)\n",
    "        \n",
    "        # FYI: average Fe/H values in Liu+ 2013 which were taken at different phases\n",
    "        # liu_feh.groupby(liu_feh['name'], axis=0, as_index=False).mean()\n",
    "        \n",
    "        # FYI: average Fe/H values in Sneden+ 1997 which were taken at different epochs\n",
    "        # sneden_feh.groupby(sneden_feh['name'], axis=0, as_index=False).mean()\n",
    "        \n",
    "        # initialize arrays: essential info\n",
    "        empir_spec_name_array = []\n",
    "        star_name_array = []\n",
    "        H_data_array = []\n",
    "        K_data_array = []\n",
    "        err_H_data_array = [] \n",
    "        err_K_data_array = []\n",
    "\n",
    "        # initialize arrays: other info\n",
    "        Hbet_data_array = []\n",
    "        err_Hbet_data_array = []\n",
    "        Hgam_data_array = []\n",
    "        err_Hgam_data_array = []\n",
    "        rHgam_data_array = [] # rescaled Hgamma\n",
    "        err_rHgam_data_array = []\n",
    "        Hdel_data_array = []\n",
    "        err_Hdel_data_array = []\n",
    "        Heps_data_array = []\n",
    "        err_Heps_data_array = []\n",
    "        \n",
    "    def __call__(self):\n",
    "        \n",
    "        # make a list of all UNIQUE, EMPIRICAL spectrum names\n",
    "        uniqueSpecNames = line_data.drop_duplicates(subset='empir_spec_name')['empir_spec_name']\n",
    "        \n",
    "        \n",
    "    # fcn: find stars that overlap with Layden 1994, and return (x,y,z)=(FeH_Lay94,FeH_input-FeH_Lay94,starname)\n",
    "    def find_match_Layden(input_table, layden_table, plot_name, offset=False):\n",
    "    \n",
    "        inputFeH = []\n",
    "        laydenFeH = []\n",
    "        nameArray = []\n",
    "    \n",
    "        for row in range(0,len(input_table)): # scan over each row in input table\n",
    "            if (layden_table['name'] == input_table['name'][row]).any():\n",
    "            #if (input_table['name'][row] isin layden_table['name']): # if there's a star name that matches\n",
    "                inputFeH = np.append(inputFeH,input_table['feh'][row])\n",
    "                laydenFeH = np.append(laydenFeH,layden_table.loc[layden_table['name'] == input_table['name'][row]]['feh'])\n",
    "                nameArray = np.append(nameArray,input_table['name'][row])\n",
    "    \n",
    "        residuals = np.subtract(inputFeH,laydenFeH)\n",
    "    \n",
    "        # best-fit line\n",
    "        coeff = np.polyfit(laydenFeH, residuals, 1)\n",
    "        limits = [-3.0,0.5]\n",
    "        line = np.multiply(coeff[0],limits)+coeff[1]\n",
    "    \n",
    "        # if there needs to be an offset (like in Fig. 6 of Chadid+ 2017)\n",
    "        chadid_y_125 = -0.10583621694962 # from Chadid line at Fe/H=-1.25\n",
    "        this_y_125 = np.multiply(coeff[0],-1.25)+coeff[1] # y-value of this line at Fe/H=-1.25\n",
    "        net_offset = chadid_y_125 - this_y_125 # offset needed to move line\n",
    "        print('Y_offset to overlap with Chadid+ 2017 at Fe/H=-1.25:')\n",
    "        print(net_offset)\n",
    "        print('Number of overlapping stars:')\n",
    "        print(len(residuals))\n",
    "        line_offset = np.add(line,net_offset)\n",
    "    \n",
    "        # save a plot\n",
    "        plt.scatter(laydenFeH, np.subtract(inputFeH,laydenFeH))\n",
    "        plt.plot([-3.0,0.5], [0., 0.], linestyle='--')\n",
    "        plt.plot(limits, line)\n",
    "        plt.plot(limits, line_offset)\n",
    "        #plt.xlim([-3.0,0.5])\n",
    "        #plt.ylim([-0.6,0.6])\n",
    "        plt.xlabel('[Fe/H]_Lay94')\n",
    "        plt.ylabel('[Fe/H]_input - [Fe/H]_Lay94')\n",
    "        plt.title('residuals between '+str(plot_name)+' and Lay94\\ny=mx+b, m='+str(coeff[0])+', b='+str(coeff[1])+'\\n offset '+str(net_offset))\n",
    "        plt.savefig(plot_name+'_test_180708.png')\n",
    "        #plt.show()\n",
    "        plt.clf()\n",
    "            \n",
    "        # return \n",
    "        # 1. overlapping Layden94 values\n",
    "        # 2. FeH values from lit source\n",
    "        # 3. Residuals between 1. and 2.(see Chadid+ 2017 Figs. 5, 6, 7)\n",
    "        # 4. coefficients of best-fit line\n",
    "        # 5. offset in y to bring lit FeH values to match Chadid+ 2017 at FeH=-1.25 (see Chadid+ 2017 Figs. 5, 6)\n",
    "        # 6. Residuals (from 3.) minus the offset (from 5.)  (see Chadid+ 2017 Fig. 7)\n",
    "        # 7. The names of the stars (in same order as arrays for 1., 2., 3., 4.)\n",
    "        \n",
    "        d = dict()\n",
    "        d['laydenFeH'] = laydenFeH\n",
    "        d['inputFeH'] = inputFeH\n",
    "        d['residuals'] = residuals\n",
    "        d['coeff'] = coeff\n",
    "        d['net_offset'] = net_offset\n",
    "        d['residuals_shifted'] = np.subtract(inputFeH,residuals)\n",
    "        d['name'] = nameArray\n",
    "        \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now actually find the matches between datasets and apply the offsets\n",
    "\n",
    "# find matches: Fernley 1996 ## ## WAIT-- FERNLEY 97 INCLUDES THESE\n",
    "#dict_Fernley_96 = lit_metallicities.find_match_Layden(fernley_feh,layden_feh,'Fernley_96', offset=True)\n",
    "\n",
    "# find matches: Lambert 1996\n",
    "dict_Lambert_96 = lit_metallicities.find_match_Layden(lambert_logeps,layden_feh,'Lambert_96', offset=True)\n",
    "\n",
    "# find matches: Nemec 2013\n",
    "dict_Nemec_2013  = lit_metallicities.find_match_Layden(nemec_feh,layden_feh,'Nemec_2013', offset=True)\n",
    "\n",
    "# find matches: Liu 2013\n",
    "liu_feh2 = liu_feh.groupby(liu_feh['name'], axis=0, as_index=False).mean()\n",
    "dict_Liu_2013  = lit_metallicities.find_match_Layden(liu_feh2,layden_feh,'Liu_2013', offset=True)\n",
    "\n",
    "# find matches: Chadid 2017\n",
    "dict_Chadid_2017  = lit_metallicities.find_match_Layden(chadid_feh,layden_feh,'Chadid_2017', offset=True)\n",
    "\n",
    "# find matches: Fernley 1997\n",
    "dict_Fernley_1997  = lit_metallicities.find_match_Layden(fernley97_feh,layden_feh,'Fernley_1997', offset=True)\n",
    "\n",
    "# find matches: Solano 1997\n",
    "dict_Solano_1997  = lit_metallicities.find_match_Layden(solano_feh,layden_feh,'Solano_1997', offset=True)\n",
    "\n",
    "## ## SNEDEN_17 DOES NOT OVERLAP WITH LAYDEN! FIX THIS\n",
    "\n",
    "# find matches: Wallerstein+ 2010\n",
    "dict_Wallerstein_2010  = lit_metallicities.find_match_Layden(wallerstein_feh,layden_feh,'Wallerstein_2010', offset=True)\n",
    "\n",
    "## ## IS THE BELOW NEEDED?\n",
    "# find matches between Wallerstein and Chadid\n",
    "# Chadid stars that appear in Wallerstein\n",
    "#chadid_winnow = lit_metallicities.chadid_feh[chadid_feh['star'].isin(wallerstein_feh['star'])]\n",
    "# Wallerstein stars that appear in Chadid\n",
    "#wallerstein_winnow = lit_metallicities.wallerstein_feh[wallerstein_feh['star'].isin(chadid_feh['star'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## APPLY OFFSETS to all datasets and overlap\n",
    "\n",
    "plt.plot(dict_Lambert_96['laydenFeH'], dict_Lambert_96['residuals_shifted'])\n",
    "plt.plot(dict_Nemec_2013['laydenFeH'], dict_Nemec_2013['residuals_shifted'])\n",
    "plt.plot(dict_Liu_2013['laydenFeH'], dict_Liu_2013['residuals_shifted'])\n",
    "plt.plot(dict_Chadid_2017['laydenFeH'], dict_Chadid_2017['residuals_shifted'])\n",
    "plt.plot(dict_Fernley_1997['laydenFeH'], dict_Fernley_1997['residuals_shifted'])\n",
    "plt.plot(dict_Solano_1997['laydenFeH'], dict_Solano_1997['residuals_shifted'])\n",
    "plt.plot(dict_Wallerstein_2010['laydenFeH'], dict_Wallerstein_2010['residuals_shifted'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge the metallicity dictionaries\n",
    "\n",
    "dict_collect = [dict_Lambert_96, dict_Nemec_2013, dict_Liu_2013, dict_Chadid_2017, \n",
    "            dict_Fernley_1997, dict_Solano_1997, dict_Wallerstein_2010]\n",
    "dict_merged = {}\n",
    "for k in dict_Lambert_96.iterkeys():\n",
    "    dict_merged[k] = tuple(dict_merged[k] for dict_merged in dict_collect)\n",
    "    \n",
    "## ## CAUTION: TEST TO SEE IF THE CONTENT IN THE KEYS IS IN ORDER (I.E., MAKE A PLOT AND SEE IF ITS THE SAME IF DATASETS ARE OVERLAID INDIVIDUALLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot merged data and fit linreg line\n",
    "\n",
    "m_merged,b_merged = polyfit(dict_merged['laydenFeH'], dict_merged['residuals_shifted'], 1)\n",
    "plt.scatter(dict_merged['laydenFeH'], dict_merged['residuals_shifted'])\n",
    "plt.plot(dict_merged['laydenFeH'], np.add(np.multiply(dict_merged['laydenFeH'],m_merged),b_merged))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CALCULATE FINAL FEH VALUES FOR OUR OWN STARS, AND WRITE OUT \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FOR EMCEE, USE CODE rrlyrae_metal_fit_emcee_wrapper.py IN SAME REPOSITORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THEN RUN EMCEE WITH FINAL EWS, ERR_EWS, FEHS, ERR_FEHS\n",
    "\n",
    "class run_emcee():\n",
    "    \n",
    "    ##############################################################################\n",
    "    # STEP 5: RUN EMCEE ON THE SPACE, GET VALUES FOR a, b, c, d (applicable only to A)\n",
    "    ##############################################################################\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "    def __call__(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in Ca absorption maps\n",
    "\n",
    "image, header = fits.getdata('maps_EW(CaNa)_20150318.fits',0,header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTENSION= 'BINTABLE'           / binary table extension                         BITPIX  =                    8 / array data type                                NAXIS   =                    2 / number of array dimensions                     NAXIS1  =                   56 / length of dimension 1                          NAXIS2  =                 3072 / length of dimension 2                          PCOUNT  =                    0 / number of group parameters                     GCOUNT  =                    1 / number of groups                               TFIELDS =                   11 / number of table fields                         TTYPE1  = 'Ca_K_EW '                                                            TFORM1  = 'E       '                                                            TTYPE2  = 'Ca_K_err'                                                            TFORM2  = 'E       '                                                            TTYPE3  = 'Ca_H_EW '                                                            TFORM3  = 'E       '                                                            TTYPE4  = 'Ca_H_err'                                                            TFORM4  = 'E       '                                                            TTYPE5  = 'Na_D2_EW'                                                            TFORM5  = 'E       '                                                            TTYPE6  = 'Na_D2_err'                                                           TFORM6  = 'E       '                                                            TTYPE7  = 'Na_D1_EW'                                                            TFORM7  = 'E       '                                                            TTYPE8  = 'Na_D1_err'                                                           TFORM8  = 'E       '                                                            TTYPE9  = 'Nspec   '                                                            TFORM9  = 'D       '                                                            TTYPE10 = 'longitude'                                                           TFORM10 = 'D       '                                                            TTYPE11 = 'latitude'                                                            TFORM11 = 'D       '                                                            NSIDE   =                   16                                                  END                                                                                                                                                                                                                                                                                                                                                                                                             \n"
     ]
    }
   ],
   "source": [
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTENSION\n",
      "BITPIX\n",
      "NAXIS\n",
      "NAXIS1\n",
      "NAXIS2\n",
      "PCOUNT\n",
      "GCOUNT\n",
      "TFIELDS\n",
      "TTYPE1\n",
      "TFORM1\n",
      "TTYPE2\n",
      "TFORM2\n",
      "TTYPE3\n",
      "TFORM3\n",
      "TTYPE4\n",
      "TFORM4\n",
      "TTYPE5\n",
      "TFORM5\n",
      "TTYPE6\n",
      "TFORM6\n",
      "TTYPE7\n",
      "TFORM7\n",
      "TTYPE8\n",
      "TFORM8\n",
      "TTYPE9\n",
      "TFORM9\n",
      "TTYPE10\n",
      "TFORM10\n",
      "TTYPE11\n",
      "TFORM11\n",
      "NSIDE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(i) for i in header]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.12263525  0.10066336  0.15257747 ...,  0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(image['Ca_K_EW '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
